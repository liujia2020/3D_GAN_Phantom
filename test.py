import os
from options.test_options import TestOptions
from data import create_dataset
from models import create_model
from util.util import save_nii  # å‡è®¾ util é‡Œæœ‰è¿™ä¸ªå‡½æ•°ï¼Œæˆ–è€…ä½ è‡ªå·±å†™çš„
import torch
import numpy as np
import nibabel as nib

def save_nii_custom(data, path):
    # ç®€æ˜“ä¿å­˜å‡½æ•°ï¼Œé˜²æ­¢ util é‡Œæ²¡æœ‰
    data = data.squeeze().cpu().numpy()
    # é€†å½’ä¸€åŒ– (å¯é€‰ï¼Œçœ‹ä½ æ˜¯å¦éœ€è¦è¿˜åŸå› dB)
    # data = (data / 2.0 + 0.5) * 60 - 60
    
    # ç¡®ä¿ä¿å­˜ç»´åº¦é¡ºåº (å¦‚æœæ˜¯ nibabel ä¹ æƒ¯ x,y,z)
    if len(data.shape) == 3:
        data = data.transpose(2, 1, 0) # D,H,W -> W,H,D for ITK-SNAP
        
    new_image = nib.Nifti1Image(data, affine=np.eye(4))
    nib.save(new_image, path)

if __name__ == '__main__':
    opt = TestOptions().parse()  # get test options
    # hardcode some parameters for test
    opt.num_threads = 0   # test code only supports num_threads = 0
    opt.batch_size = 1    # test code only supports batch_size = 1
    opt.serial_batches = True  # disable data shuffling; comment this line if results on randomly chosen images are needed.
    opt.no_flip = True    # no flip for test

    dataset = create_dataset(opt)  # create a dataset given opt.dataset_mode and other options
    model = create_model(opt)      # create a model given opt.model and other options
    model.setup(opt)               # regular setup: load and print networks; create schedulers

    if opt.eval:
        model.eval()
        
    print(f"Testing experiment: {opt.name}")
    
    for i, data in enumerate(dataset):
        if i >= opt.num_test:  # only apply our model to opt.num_test images.
            break
            
        model.set_input(data)  # unpack data from data loader
        model.test()           # run inference
        visuals = model.get_current_visuals()  # get image results
        
        img_path = model.get_image_paths()     # get image paths
        short_path = os.path.basename(img_path[0])
        name = os.path.splitext(short_path)[0]
        
        # === [æ ¸å¿ƒä¿®æ”¹] æ–‡ä»¶ååŠ ä¸Š opt.name ===
        # åŸæ¥: Case001_fake.nii
        # ç°åœ¨: Case001_fake_experiment_v1.nii
        
        fake_im = visuals['fake_hq'] # å‡è®¾ä½ çš„æ¨¡å‹è¾“å‡ºå« fake_hq
        
        # ç»“æœç›®å½•
        res_dir = os.path.join(opt.results_dir, opt.name)
        if not os.path.exists(res_dir):
            os.makedirs(res_dir)
            
        save_name = f"{name}_fake_{opt.name}.nii"
        save_path = os.path.join(res_dir, save_name)
        
        print(f"Saving {save_path} ...")
        save_nii_custom(fake_im, save_path)

# import os
# import torch
# import numpy as np
# import matplotlib
# matplotlib.use('Agg') 
# import matplotlib.pyplot as plt
# import nibabel as nib
# from tqdm import tqdm
# import pandas as pd
# import json
# import logging

# from options.test_options import TestOptions
# from data import create_dataset
# from models import create_model
# from utils.metrics import calc_metrics

# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# def save_visual_matrix_png(lq_avg, fake, hq, sq, case_name, save_dir, opt):
#     """
#     V4.5 æ ¸å¿ƒï¼šç”Ÿæˆ 4è¡Œx3åˆ— çš„å…¨æ™¯ä¸‰è§†å›¾çŸ©é˜µ (LQ, Fake, HQ, SQ)
#     å¹¶è‡ªåŠ¨æ ¡æ­£ç‰©ç†é•¿å®½æ¯”
#     """
#     os.makedirs(os.path.join(save_dir, 'images'), exist_ok=True)
    
#     # å‡†å¤‡æ•°æ®å­—å…¸
#     data_map = {
#         'Input (LQ)': lq_avg,
#         'Generated (Fake)': fake,
#         'Ref (HQ)': hq,
#         'Target (SQ)': sq
#     }
    
#     # è·å–ä¸­å¿ƒåˆ‡ç‰‡ç´¢å¼•
#     d_mid, h_mid, w_mid = lq_avg.shape[0] // 2, lq_avg.shape[1] // 2, lq_avg.shape[2] // 2
    
#     # è®¾ç½®ç»˜å›¾å¸ƒå±€: 4è¡Œ (æ•°æ®) x 3åˆ— (è§†å›¾)
#     fig, axes = plt.subplots(4, 3, figsize=(15, 20), constrained_layout=True)
    
#     # è®¡ç®—é•¿è½´çš„æ˜¾ç¤ºæ¯”ä¾‹ (é˜²æ­¢å›¾åƒè¢«æ‹‰ä¼¸æˆé¢æ¡)
#     # åƒç´ æ¯”ä¾‹æ˜¯ 0.0362 / 0.2 â‰ˆ 0.181
#     aspect_ratio_long = opt.spacing_z / opt.spacing_x 
    
#     for row_idx, (name, data) in enumerate(data_map.items()):
#         # 1. Axial View (æ¨ªæˆªé¢): Slice Dim 0 (D)
#         ax_axial = axes[row_idx, 0]
#         slice_axial = data[d_mid, :, :]
#         # å¯¹æ•°å¢å¼ºæ˜¾ç¤º
#         img_ax = 20 * np.log10(np.maximum(slice_axial, 1e-6))
#         ax_axial.imshow(img_ax, cmap='gray', aspect='equal')
#         if row_idx == 0: ax_axial.set_title("Axial (H-W)\nCross-Section", fontsize=14, fontweight='bold')
#         ax_axial.set_ylabel(name, fontsize=16, fontweight='bold')
#         ax_axial.set_xticks([])
#         ax_axial.set_yticks([])

#         # 2. Coronal View (å† çŠ¶é¢): Slice Dim 1 (H)
#         ax_cor = axes[row_idx, 1]
#         slice_cor = data[:, h_mid, :] 
#         img_cor = 20 * np.log10(np.maximum(slice_cor, 1e-6))
#         ax_cor.imshow(img_cor, cmap='gray', aspect=aspect_ratio_long)
#         if row_idx == 0: ax_cor.set_title("Coronal (D-W)\nLongitudinal", fontsize=14, fontweight='bold')
#         ax_cor.set_xticks([])
#         ax_cor.set_yticks([])

#         # 3. Sagittal View (çŸ¢çŠ¶é¢): Slice Dim 2 (W)
#         ax_sag = axes[row_idx, 2]
#         slice_sag = data[:, :, w_mid]
#         img_sag = 20 * np.log10(np.maximum(slice_sag, 1e-6))
#         ax_sag.imshow(img_sag, cmap='gray', aspect=aspect_ratio_long)
#         if row_idx == 0: ax_sag.set_title("Sagittal (D-H)\nLongitudinal", fontsize=14, fontweight='bold')
#         ax_sag.set_xticks([])
#         ax_sag.set_yticks([])

#     # ä¿å­˜é«˜æ¸…å¤§å›¾
#     save_path = os.path.join(save_dir, 'images', f'{case_name}_Matrix_View.png')
#     plt.savefig(save_path, dpi=150)
#     plt.close(fig)
#     logging.info(f"ä¸‰è§†å›¾çŸ©é˜µå·²ä¿å­˜: {save_path}")

# def save_full_volume_nifti_cloned(data, template_path, name, suffix, save_dir):
#     """V4.3: å…‹éš†æ¨¡æ¿å…ƒæ•°æ®ä¿å­˜ NIfTI"""
#     os.makedirs(os.path.join(save_dir, 'nifti'), exist_ok=True)
#     try:
#         template_img = nib.load(template_path)
#         affine = template_img.affine
#         header = template_img.header
#     except:
#         affine = np.eye(4)
#         header = None
        
#     nii_img = nib.Nifti1Image(data, affine, header)
#     save_path = os.path.join(save_dir, 'nifti', f'{name}_{suffix}.nii')
#     nib.save(nii_img, save_path)
#     logging.info(f"NIfTIæ–‡ä»¶(æ–¹å‘å·²æ ¡æ­£)å·²ä¿å­˜: {save_path}")

# def denormalize_output(normalized_data, max_val):
#     """
#     [ä¿®æ­£] ä½¿ç”¨å›ºå®šçš„ç‰©ç† Max å€¼è¿›è¡Œåå½’ä¸€åŒ–
#     ä¸å†ä¾èµ–å¤–éƒ¨ jsonï¼Œç¡®ä¿è¾“å‡ºæ•°å€¼æ˜¯çœŸå®çš„ç‰©ç†å¼ºåº¦
#     """
#     data_0_1 = (normalized_data + 1.0) / 2.0
#     return data_0_1 * max_val

# # def full_inference_8gb(model, input_tensor):
# #     """V4.1: 512åˆ†æ®µæ¨ç† (æ˜¾å­˜ä¼˜åŒ–) - ä¿æŒä¸å˜"""
# #     _, _, D, H, W = input_tensor.shape
# #     mid = 512
# #     output_full = np.zeros((D, H, W), dtype=np.float32)
# #     model.netG.cuda()
# #     with torch.no_grad():
# #         # Part 1
# #         part1_in = input_tensor[:, :, :mid, :, :].cuda()
# #         output_full[:mid, :, :] = model.netG(part1_in).squeeze().cpu().numpy()
# #         del part1_in
# #         torch.cuda.empty_cache()
# #         # Part 2
# #         part2_in = input_tensor[:, :, mid:, :, :].cuda()
# #         output_full[mid:, :, :] = model.netG(part2_in).squeeze().cpu().numpy()
# #         del part2_in
# #         torch.cuda.empty_cache()
# #     return output_full


# def full_inference_overlap(model, input_tensor, overlap=64):
#     """
#     [V5.0 - 4060 8GB ä¸“ç”¨ç‰ˆ] å¸¦é‡å çš„åˆ‡åˆ†æ¨ç†
#     è§£å†³ "ä¸­é—´æœ‰ä¸€æ¡çº¿" çš„è¾¹ç•Œä¼ªå½±é—®é¢˜ã€‚
#     åŸç†ï¼šå¤šç®— 64 å±‚ (Overlap) ä½œä¸º Paddingï¼Œæ‹¼æ¥æ—¶ä¸¢å¼ƒè¾¹ç¼˜ã€‚
#     """
#     b, c, D, H, W = input_tensor.shape
    
#     # 1. æ˜¾å­˜ä¸å¤Ÿæ—¶çš„è‡ªåŠ¨ç­–ç•¥ï¼šå¦‚æœæ·±åº¦ < 600ï¼Œç›´æ¥è·‘ (8GB åº”è¯¥èƒ½å‹‰å¼ºåƒä¸‹ 600)
#     # å¦‚æœæ·±åº¦å¤ªæ·± (1024)ï¼Œåˆ™å¯åŠ¨åˆ‡åˆ†
#     if D < 600:
#         model.netG.cuda()
#         with torch.no_grad():
#             output_full = model.netG(input_tensor.cuda()).squeeze().cpu().numpy()
#         return output_full

#     # 2. å‡†å¤‡è¾“å‡ºå®¹å™¨
#     output_full = np.zeros((D, H, W), dtype=np.float32)
#     model.netG.cuda()
    
#     mid = D // 2 # 512
    
#     with torch.no_grad():
#         # ================= Part 1: Top Half =================
#         # è¾“å…¥èŒƒå›´: 0 ~ (512 + overlap)
#         # ç›®çš„: åªè¦ 0 ~ 512 çš„çº¯å‡€æ•°æ®
#         end_idx = min(D, mid + overlap)
#         part1_in = input_tensor[:, :, :end_idx, :, :].cuda()
        
#         # æ¨ç†
#         out1 = model.netG(part1_in).squeeze().cpu().numpy()
        
#         # è£å‰ª: åªè¦ [0:mid]
#         output_full[:mid, :, :] = out1[:mid, :, :]
        
#         # æ¸…æ˜¾å­˜
#         del part1_in, out1
#         torch.cuda.empty_cache()

#         # ================= Part 2: Bottom Half =================
#         # è¾“å…¥èŒƒå›´: (512 - overlap) ~ 1024
#         # ç›®çš„: åªè¦ 512 ~ 1024 çš„çº¯å‡€æ•°æ®
#         start_idx = max(0, mid - overlap)
#         part2_in = input_tensor[:, :, start_idx:, :, :].cuda()
        
#         # æ¨ç†
#         out2 = model.netG(part2_in).squeeze().cpu().numpy()
        
#         # è£å‰ª: è¿™é‡Œçš„ out2 é•¿åº¦æ˜¯ (1024 - start_idx)
#         # ä¹Ÿå°±æ˜¯ (1024 - (512 - overlap)) = 512 + overlap
#         # æˆ‘ä»¬çš„æœ‰æ•ˆæ•°æ®æ˜¯ä» overlap å¼€å§‹çš„ï¼Œå¯¹åº”çš„ç»å¯¹ä½ç½®æ˜¯ 512
#         valid_start_rel = mid - start_idx # ç†è®ºä¸Šç­‰äº overlap
        
#         output_full[mid:, :, :] = out2[valid_start_rel:, :, :]
        
#         # æ¸…æ˜¾å­˜
#         del part2_in, out2
#         torch.cuda.empty_cache()
        
#     return output_full

# if __name__ == '__main__':
#     opt = TestOptions().parse()
#     opt.num_threads = 0
#     opt.batch_size = 1
    
#     model = create_model(opt)
#     model.setup(opt)
#     model.eval()
    
#     dataset = create_dataset(opt)
#     save_root = os.path.join(opt.results_dir, opt.name, f"epoch_{opt.epoch}")
#     os.makedirs(save_root, exist_ok=True)
    
#     logging.info(f"ğŸš€ V4.6 ä¿®å¤ç‰ˆæµ‹è¯•å¯åŠ¨ | å‚æ•°ä¼ é€’ä¿®å¤ | ç‰©ç†æ•°å€¼æ ¡æ­£")
    
#     file_list = dataset.dataset.file_list
#     metrics_list = []

#     # ç›´æ¥ä» Dataset è·å–æ­£ç¡®çš„ç‰©ç†å®šæ ‡å€¼
#     NORM_MAX_INPUT = dataset.dataset.NORM_MAX_INPUT    # 500,000.0
#     NORM_MAX_TARGET = dataset.dataset.NORM_MAX_TARGET  # 25,000,000.0

#     for i, files in enumerate(tqdm(file_list)):
#         case_name = files['case_name']
#         template_sq_path = files['p_sq']
        
#         # 1. æ ¸å¿ƒæ¨ç†
#         # [å…³é”®ä¿®å¤] å‚æ•°é¡ºåºä¿®æ­£: path, slice_objs, max_val
#         # è¿™é‡Œçš„ slice(None) è¡¨ç¤ºè¯»å–æ•´ä¸ª 1024x128x128 æ•°æ®
#         full_slice = (slice(None), slice(None), slice(None))
        
#         input_data = [dataset.dataset._read_and_process(files[k], full_slice, NORM_MAX_INPUT) 
#                       for k in ['p_in_n15', 'p_in_000', 'p_in_p15']]
        
#         real_lq_tensor = torch.from_numpy(np.stack(input_data, axis=0)).unsqueeze(0).float()
        
#         # æ­¤æ—¶ real_lq_tensor æ·±åº¦åº”è¯¥æ˜¯ 1024ï¼Œinference å‡½æ•°å°†æ­£å¸¸å·¥ä½œ
#         # fake_norm = full_inference_overlap(model, real_lq_tensor)
#         fake_norm = full_inference_overlap(model, real_lq_tensor, overlap=64)

#         # 2. åå½’ä¸€åŒ– (ä½¿ç”¨ 2.5e7 çš„åŸºå‡†ï¼Œç¡®ä¿è¾“å‡ºæ˜¯ç™¾ä¸‡çº§æ•°å€¼)
#         fake_denorm = denormalize_output(fake_norm, NORM_MAX_TARGET)
        
#         # çœŸå€¼ä¹Ÿç”¨åŒæ ·çš„åŸºå‡†è¿˜åŸ
#         sq_denorm = denormalize_output(dataset.dataset._read_and_process(files['p_sq'], full_slice, NORM_MAX_TARGET), NORM_MAX_TARGET)
#         hq_denorm = denormalize_output(dataset.dataset._read_and_process(files['p_hq'], full_slice, NORM_MAX_TARGET), NORM_MAX_TARGET)
        
#         # Input ä¹Ÿè¦è¿˜åŸ (æ³¨æ„ Input çš„åŸºå‡†æ˜¯ 5e5)
#         lq_avg_norm = np.mean(np.stack(input_data, axis=0), axis=0)
#         lq_avg_denorm = denormalize_output(lq_avg_norm, NORM_MAX_INPUT)

#         # 3. æŒ‡æ ‡è®¡ç®—
#         m_sq = calc_metrics(torch.from_numpy(fake_denorm).unsqueeze(0).unsqueeze(0), torch.from_numpy(sq_denorm).unsqueeze(0).unsqueeze(0))
#         m_hq = calc_metrics(torch.from_numpy(fake_denorm).unsqueeze(0).unsqueeze(0), torch.from_numpy(hq_denorm).unsqueeze(0).unsqueeze(0))

#         # 4. ä¿å­˜æ–‡ä»¶
#         save_full_volume_nifti_cloned(fake_denorm, template_sq_path, case_name, 'Fake', save_root)
        
#         # 5. ä¿å­˜å¯è§†åŒ– (ä½¿ç”¨è¿˜åŸåçš„ç‰©ç†æ•°å€¼ï¼Œlogæ˜¾ç¤ºä¼šå¾ˆæ¸…æ™°)
#         save_visual_matrix_png(lq_avg_denorm, fake_denorm, hq_denorm, sq_denorm, case_name, save_root, opt)
        
#         metrics_list.append({'Name': case_name, 'PSNR_SQ': m_sq['PSNR'], 'SSIM_SQ': m_sq['SSIM'], 'PSNR_HQ': m_hq['PSNR'], 'SSIM_HQ': m_hq['SSIM']})

#     pd.DataFrame(metrics_list).to_csv(os.path.join(save_root, 'metrics_final.csv'), index=False)
#     logging.info(f"âœ… å…¨éƒ¨å®Œæˆï¼è¾“å‡ºæ–‡ä»¶å·²ç”Ÿæˆåœ¨ {save_root}")