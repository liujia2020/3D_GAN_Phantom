# # import os
# # import torch
# # import numpy as np
# # import nibabel as nib
# # from tqdm import tqdm
# # import pandas as pd
# # import logging
# # from options.test_options import TestOptions
# # from data import create_dataset
# # from models import create_model
# # from utils.metrics import calc_metrics

# # # é…ç½® Logging
# # logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# # def save_full_volume_nifti_cloned(data, template_path, name, suffix, save_dir):
# #     """ä¿å­˜ NIfTI"""
# #     os.makedirs(os.path.join(save_dir, 'nifti'), exist_ok=True)
# #     try:
# #         template_img = nib.load(template_path)
# #         affine = template_img.affine
# #         header = template_img.header
# #     except:
# #         # å¦‚æœæ¨¡æ¿åäº†ï¼Œç”¨é»˜è®¤çŸ©é˜µ (åº”å¯¹æç«¯æƒ…å†µ)
# #         affine = np.eye(4)
# #         header = None
        
# #     nii_img = nib.Nifti1Image(data, affine, header)
# #     save_path = os.path.join(save_dir, 'nifti', f'{name}_{suffix}.nii')
# #     nib.save(nii_img, save_path)

# # def denormalize_output(normalized_data, max_val):
# #     """åå½’ä¸€åŒ–: [-1, 1] -> åŸå§‹æ•°å€¼"""
# #     data_0_1 = (normalized_data + 1.0) / 2.0
# #     return data_0_1 * max_val

# # # === [æ ¸å¿ƒä¿®æ”¹] ç›´é€šå¼æ¨ç†å‡½æ•° ===
# # def full_inference_direct(model, input_tensor):
# #     """
# #     ä¸åˆ‡åˆ†ã€ä¸åˆ†å—ã€‚
# #     ç›´æ¥æŠŠ 1024x128x128 æ‰”è¿›æ˜¾å¡ã€‚
# #     """
# #     model.netG.cuda()
# #     with torch.no_grad():
# #         # ç›´æ¥æ¨ç†ï¼Œæ²¡æœ‰ä»»ä½•èŠ±å“¨çš„æ“ä½œ
# #         output_full = model.netG(input_tensor.cuda()).squeeze().cpu().numpy()
# #     return output_full

# # if __name__ == '__main__':
# #     opt = TestOptions().parse()
# #     opt.num_threads = 0
# #     opt.batch_size = 1
    
# #     model = create_model(opt)
# #     model.setup(opt)
# #     model.eval()
    
# #     # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨åˆšæ‰ä¿®æ”¹è¿‡çš„ Dataset
# #     dataset = create_dataset(opt)
# #     save_root = os.path.join(opt.results_dir, opt.name, f"epoch_{opt.epoch}")
# #     os.makedirs(save_root, exist_ok=True)
    
# #     logging.info(f"ğŸš€ ç›´é€šå¼æµ‹è¯•å¯åŠ¨ | ä¸åˆ‡åˆ† | å…¨å°ºå¯¸è¾“å…¥è¾“å‡º")
    
# #     file_list = dataset.dataset.file_list
# #     metrics_list = []

# #     # ç‰©ç†å‚æ•° (æ ¹æ®å…±è¯†)
# #     MAX_VAL_SQ = 25000000.0 

# #     for i, files in enumerate(tqdm(file_list)):
# #         case_name = files['case_name']
# #         template_sq_path = files['p_sq']
        
# #         # 1. è¯»å– (ä½¿ç”¨ dataset çš„é€»è¾‘ï¼Œè·å–å…¨å°ºå¯¸æ•°æ®)
# #         # slice(None) ä»£è¡¨è¯»å–å…¨éƒ¨ï¼Œé…åˆåˆšæ‰ä¿®æ”¹çš„ Datasetï¼Œè¿™é‡Œä¸€å®šè¿”å› 1024x128x128
# #         input_data = [dataset.dataset._read_and_process(files[k], (slice(None), slice(None), slice(None)), dataset.dataset.NORM_MAX_INPUT) 
# #                       for k in ['p_in_n15', 'p_in_000', 'p_in_p15']]
        
# #         real_lq_tensor = torch.from_numpy(np.stack(input_data, axis=0)).unsqueeze(0).float()
        
# #         # 2. æ¨ç† (ç›´æ¥è·‘)
# #         fake_norm = full_inference_direct(model, real_lq_tensor)

# #         # 3. åå½’ä¸€åŒ–
# #         fake_denorm = denormalize_output(fake_norm, MAX_VAL_SQ)
        
# #         # è¯»å–çœŸå€¼ç”¨äºå¯¹æ¯”
# #         sq_denorm = denormalize_output(dataset.dataset._read_and_process(files['p_sq'], (slice(None), slice(None), slice(None)), dataset.dataset.NORM_MAX_TARGET), MAX_VAL_SQ)
# #         hq_denorm = denormalize_output(dataset.dataset._read_and_process(files['p_hq'], (slice(None), slice(None), slice(None)), dataset.dataset.NORM_MAX_TARGET), MAX_VAL_SQ)

# #         # 4. æŒ‡æ ‡è®¡ç®—
# #         m_sq = calc_metrics(torch.from_numpy(fake_denorm).unsqueeze(0).unsqueeze(0), torch.from_numpy(sq_denorm).unsqueeze(0).unsqueeze(0))
# #         m_hq = calc_metrics(torch.from_numpy(fake_denorm).unsqueeze(0).unsqueeze(0), torch.from_numpy(hq_denorm).unsqueeze(0).unsqueeze(0))

# #         # 5. ä¿å­˜ NIfTI
# #         save_full_volume_nifti_cloned(fake_denorm, template_sq_path, case_name, 'Fake', save_root)
        
# #         metrics_list.append({'Name': case_name, 'PSNR_SQ': m_sq['PSNR'], 'SSIM_SQ': m_sq['SSIM'], 'PSNR_HQ': m_hq['PSNR'], 'SSIM_HQ': m_hq['SSIM']})

# #     pd.DataFrame(metrics_list).to_csv(os.path.join(save_root, 'metrics_final.csv'), index=False)
# #     logging.info(f"âœ… æµ‹è¯•å®Œæˆã€‚")

# import os
# import torch
# import numpy as np
# import matplotlib
# matplotlib.use('Agg') 
# import matplotlib.pyplot as plt
# import nibabel as nib
# from tqdm import tqdm
# import pandas as pd
# import json
# import logging

# from options.test_options import TestOptions
# from data import create_dataset
# from models import create_model
# from utils.metrics import calc_metrics

# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# def save_visual_matrix_png(lq_avg, fake, hq, sq, case_name, save_dir, opt):
#     """
#     V4.5 æ ¸å¿ƒï¼šç”Ÿæˆ 4è¡Œx3åˆ— çš„å…¨æ™¯ä¸‰è§†å›¾çŸ©é˜µ (LQ, Fake, HQ, SQ)
#     å¹¶è‡ªåŠ¨æ ¡æ­£ç‰©ç†é•¿å®½æ¯”
#     """
#     os.makedirs(os.path.join(save_dir, 'images'), exist_ok=True)
    
#     # å‡†å¤‡æ•°æ®å­—å…¸
#     data_map = {
#         'Input (LQ)': lq_avg,
#         'Generated (Fake)': fake,
#         'Ref (HQ)': hq,
#         'Target (SQ)': sq
#     }
    
#     # è·å–ä¸­å¿ƒåˆ‡ç‰‡ç´¢å¼•
#     d_mid, h_mid, w_mid = lq_avg.shape[0] // 2, lq_avg.shape[1] // 2, lq_avg.shape[2] // 2
    
#     # è®¾ç½®ç»˜å›¾å¸ƒå±€: 4è¡Œ (æ•°æ®) x 3åˆ— (è§†å›¾)
#     fig, axes = plt.subplots(4, 3, figsize=(15, 20), constrained_layout=True)
    
#     # è®¡ç®—é•¿è½´çš„æ˜¾ç¤ºæ¯”ä¾‹ (é˜²æ­¢å›¾åƒè¢«æ‹‰ä¼¸æˆé¢æ¡)
#     # åƒç´ æ¯”ä¾‹æ˜¯ 0.0362 / 0.2 â‰ˆ 0.181
#     # æ„å‘³ç€ Z è½´æ¯ 1 ä¸ªåƒç´ çš„é«˜åº¦ï¼Œåªç›¸å½“äº X è½´ 0.18 ä¸ªåƒç´ çš„å®½åº¦
#     aspect_ratio_long = opt.spacing_z / opt.spacing_x 
    
#     row_names = list(data_map.keys())
    
#     for row_idx, (name, data) in enumerate(data_map.items()):
#         # 1. Axial View (æ¨ªæˆªé¢): Slice Dim 0 (D)
#         # å½¢çŠ¶: (128, 128) -> æ­£æ–¹å½¢
#         ax_axial = axes[row_idx, 0]
#         slice_axial = data[d_mid, :, :]
#         # å¯¹æ•°å¢å¼ºæ˜¾ç¤º
#         img_ax = 20 * np.log10(np.maximum(slice_axial, 1e-6))
#         ax_axial.imshow(img_ax, cmap='gray', aspect='equal')
#         if row_idx == 0: ax_axial.set_title("Axial (H-W)\nCross-Section", fontsize=14, fontweight='bold')
#         ax_axial.set_ylabel(name, fontsize=16, fontweight='bold')
#         ax_axial.set_xticks([])
#         ax_axial.set_yticks([])

#         # 2. Coronal View (å† çŠ¶é¢): Slice Dim 1 (H)
#         # å½¢çŠ¶: (1024, 128) -> é•¿æ¡å½¢
#         ax_cor = axes[row_idx, 1]
#         slice_cor = data[:, h_mid, :] 
#         img_cor = 20 * np.log10(np.maximum(slice_cor, 1e-6))
#         # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬è®© Z è½´ç«–ç€æ”¾ï¼Œæ‰€ä»¥ aspect è®¾ä¸ºè®¡ç®—å‡ºçš„æ¯”ä¾‹
#         ax_cor.imshow(img_cor, cmap='gray', aspect=aspect_ratio_long)
#         if row_idx == 0: ax_cor.set_title("Coronal (D-W)\nLongitudinal", fontsize=14, fontweight='bold')
#         ax_cor.set_xticks([])
#         ax_cor.set_yticks([])

#         # 3. Sagittal View (çŸ¢çŠ¶é¢): Slice Dim 2 (W)
#         # å½¢çŠ¶: (1024, 128) -> é•¿æ¡å½¢
#         ax_sag = axes[row_idx, 2]
#         slice_sag = data[:, :, w_mid]
#         img_sag = 20 * np.log10(np.maximum(slice_sag, 1e-6))
#         ax_sag.imshow(img_sag, cmap='gray', aspect=aspect_ratio_long)
#         if row_idx == 0: ax_sag.set_title("Sagittal (D-H)\nLongitudinal", fontsize=14, fontweight='bold')
#         ax_sag.set_xticks([])
#         ax_sag.set_yticks([])

#     # ä¿å­˜é«˜æ¸…å¤§å›¾
#     save_path = os.path.join(save_dir, 'images', f'{case_name}_Matrix_View.png')
#     plt.savefig(save_path, dpi=150) # æé«˜åˆ†è¾¨ç‡
#     plt.close(fig)
#     logging.info(f"ä¸‰è§†å›¾çŸ©é˜µå·²ä¿å­˜: {save_path}")

# def save_full_volume_nifti_cloned(data, template_path, name, suffix, save_dir):
#     """V4.3: å…‹éš†æ¨¡æ¿å…ƒæ•°æ®ä¿å­˜ NIfTI"""
#     os.makedirs(os.path.join(save_dir, 'nifti'), exist_ok=True)
#     template_img = nib.load(template_path)
#     # ä¿æŒ (1024, 128, 128)ï¼Œç›´æ¥å¥—ç”¨æ¨¡æ¿ Affine
#     nii_img = nib.Nifti1Image(data, template_img.affine, template_img.header)
#     save_path = os.path.join(save_dir, 'nifti', f'{name}_{suffix}.nii')
#     nib.save(nii_img, save_path)
#     logging.info(f"NIfTIæ–‡ä»¶(æ–¹å‘å·²æ ¡æ­£)å·²ä¿å­˜: {save_path}")

# def denormalize_output(normalized_data, case_file_name, max_dict, default_max=1.0):
#     """V4.2: æ•°å€¼åå½’ä¸€åŒ–"""
#     data_0_1 = (normalized_data + 1.0) / 2.0
#     global_max = max_dict.get(case_file_name, default_max)
#     return data_0_1 * global_max

# def full_inference_8gb(model, input_tensor):
#     """V4.1: 512åˆ†æ®µæ¨ç† (æ˜¾å­˜ä¼˜åŒ–)"""
#     _, _, D, H, W = input_tensor.shape
#     mid = 512
#     output_full = np.zeros((D, H, W), dtype=np.float32)
#     model.netG.cuda()
#     with torch.no_grad():
#         # Part 1
#         part1_in = input_tensor[:, :, :mid, :, :].cuda()
#         output_full[:mid, :, :] = model.netG(part1_in).squeeze().cpu().numpy()
#         del part1_in
#         torch.cuda.empty_cache()
#         # Part 2
#         part2_in = input_tensor[:, :, mid:, :, :].cuda()
#         output_full[mid:, :, :] = model.netG(part2_in).squeeze().cpu().numpy()
#         del part2_in
#         torch.cuda.empty_cache()
#     return output_full

# if __name__ == '__main__':
#     opt = TestOptions().parse()
#     opt.num_threads = 0
#     opt.batch_size = 1
    
#     model = create_model(opt)
#     model.setup(opt)
#     model.eval()
    
#     dataset = create_dataset(opt)
#     save_root = os.path.join(opt.results_dir, opt.name, f"epoch_{opt.epoch}")
#     os.makedirs(save_root, exist_ok=True)
    
#     # åŠ è½½å…¨å±€Maxç»Ÿè®¡
#     stats_path = os.path.join(opt.dataroot, "dataset_stats.json")
#     max_dict = {}
#     if os.path.exists(stats_path):
#         with open(stats_path, 'r') as f:
#             max_dict = json.load(f)
#             logging.info(f"âœ… å·²åŠ è½½ {len(max_dict)} æ¡å½’ä¸€åŒ–è®°å½•")

#     logging.info(f"ğŸš€ V4.5 ç»ˆææµ‹è¯•å¯åŠ¨ | çŸ©é˜µä¸‰è§†å›¾ + ç‰©ç†æ¯”ä¾‹æ ¡æ­£")
    
#     file_list = dataset.dataset.file_list
#     metrics_list = []

#     for i, files in enumerate(tqdm(file_list)):
#         case_name = files['case_name']
#         template_sq_path = files['p_sq']
        
#         # 1. æ ¸å¿ƒæ¨ç† (Output: [-1, 1])
#         input_data = [dataset.dataset._read_and_process(files[k], True, (slice(None), slice(None), slice(None))) 
#                       for k in ['p_in_n15', 'p_in_000', 'p_in_p15']]
#         real_lq_tensor = torch.from_numpy(np.stack(input_data, axis=0)).unsqueeze(0).float()
#         fake_norm = full_inference_8gb(model, real_lq_tensor)

#         # 2. åå½’ä¸€åŒ– (Output: åŸå§‹ç™¾ä¸‡çº§æ•°å€¼)
#         sq_fn = os.path.basename(files['p_sq'])
#         fake_denorm = denormalize_output(fake_norm, sq_fn, max_dict)
        
#         sq_denorm = denormalize_output(dataset.dataset._read_and_process(files['p_sq'], False, (slice(None), slice(None), slice(None))), sq_fn, max_dict)
#         hq_denorm = denormalize_output(dataset.dataset._read_and_process(files['p_hq'], False, (slice(None), slice(None), slice(None))), sq_fn, max_dict)
#         lq_avg_denorm = denormalize_output(np.mean(np.stack(input_data, axis=0), axis=0), sq_fn, max_dict)

#         # 3. æŒ‡æ ‡è®¡ç®—
#         m_sq = calc_metrics(torch.from_numpy(fake_denorm).unsqueeze(0).unsqueeze(0), torch.from_numpy(sq_denorm).unsqueeze(0).unsqueeze(0))
#         m_hq = calc_metrics(torch.from_numpy(fake_denorm).unsqueeze(0).unsqueeze(0), torch.from_numpy(hq_denorm).unsqueeze(0).unsqueeze(0))

#         # 4. ä¿å­˜æ–‡ä»¶ (åªå­˜ Fake)
#         save_full_volume_nifti_cloned(fake_denorm, template_sq_path, case_name, 'Fake', save_root)
        
#         # 5. ä¿å­˜ V4.5 çŸ©é˜µè§†å›¾ (4è¡Œ3åˆ—)
#         save_visual_matrix_png(lq_avg_denorm, fake_denorm, hq_denorm, sq_denorm, case_name, save_root, opt)
        
#         metrics_list.append({'Name': case_name, 'PSNR_SQ': m_sq['PSNR'], 'SSIM_SQ': m_sq['SSIM'], 'PSNR_HQ': m_hq['PSNR'], 'SSIM_HQ': m_hq['SSIM']})

#     pd.DataFrame(metrics_list).to_csv(os.path.join(save_root, 'metrics_final.csv'), index=False)
#     logging.info(f"âœ… å…¨éƒ¨å®Œæˆï¼è¯·æŸ¥çœ‹ images æ–‡ä»¶å¤¹ä¸‹çš„çŸ©é˜µå¤§å›¾ã€‚")

import os
import torch
import numpy as np
import matplotlib
matplotlib.use('Agg') 
import matplotlib.pyplot as plt
import nibabel as nib
from tqdm import tqdm
import pandas as pd
import json
import logging

from options.test_options import TestOptions
from data import create_dataset
from models import create_model
from utils.metrics import calc_metrics

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def save_visual_matrix_png(lq_avg, fake, hq, sq, case_name, save_dir, opt):
    """
    V4.5 æ ¸å¿ƒï¼šç”Ÿæˆ 4è¡Œx3åˆ— çš„å…¨æ™¯ä¸‰è§†å›¾çŸ©é˜µ (LQ, Fake, HQ, SQ)
    å¹¶è‡ªåŠ¨æ ¡æ­£ç‰©ç†é•¿å®½æ¯”
    """
    os.makedirs(os.path.join(save_dir, 'images'), exist_ok=True)
    
    # å‡†å¤‡æ•°æ®å­—å…¸
    data_map = {
        'Input (LQ)': lq_avg,
        'Generated (Fake)': fake,
        'Ref (HQ)': hq,
        'Target (SQ)': sq
    }
    
    # è·å–ä¸­å¿ƒåˆ‡ç‰‡ç´¢å¼•
    d_mid, h_mid, w_mid = lq_avg.shape[0] // 2, lq_avg.shape[1] // 2, lq_avg.shape[2] // 2
    
    # è®¾ç½®ç»˜å›¾å¸ƒå±€: 4è¡Œ (æ•°æ®) x 3åˆ— (è§†å›¾)
    fig, axes = plt.subplots(4, 3, figsize=(15, 20), constrained_layout=True)
    
    # è®¡ç®—é•¿è½´çš„æ˜¾ç¤ºæ¯”ä¾‹ (é˜²æ­¢å›¾åƒè¢«æ‹‰ä¼¸æˆé¢æ¡)
    # åƒç´ æ¯”ä¾‹æ˜¯ 0.0362 / 0.2 â‰ˆ 0.181
    aspect_ratio_long = opt.spacing_z / opt.spacing_x 
    
    for row_idx, (name, data) in enumerate(data_map.items()):
        # 1. Axial View (æ¨ªæˆªé¢): Slice Dim 0 (D)
        ax_axial = axes[row_idx, 0]
        slice_axial = data[d_mid, :, :]
        # å¯¹æ•°å¢å¼ºæ˜¾ç¤º
        img_ax = 20 * np.log10(np.maximum(slice_axial, 1e-6))
        ax_axial.imshow(img_ax, cmap='gray', aspect='equal')
        if row_idx == 0: ax_axial.set_title("Axial (H-W)\nCross-Section", fontsize=14, fontweight='bold')
        ax_axial.set_ylabel(name, fontsize=16, fontweight='bold')
        ax_axial.set_xticks([])
        ax_axial.set_yticks([])

        # 2. Coronal View (å† çŠ¶é¢): Slice Dim 1 (H)
        ax_cor = axes[row_idx, 1]
        slice_cor = data[:, h_mid, :] 
        img_cor = 20 * np.log10(np.maximum(slice_cor, 1e-6))
        ax_cor.imshow(img_cor, cmap='gray', aspect=aspect_ratio_long)
        if row_idx == 0: ax_cor.set_title("Coronal (D-W)\nLongitudinal", fontsize=14, fontweight='bold')
        ax_cor.set_xticks([])
        ax_cor.set_yticks([])

        # 3. Sagittal View (çŸ¢çŠ¶é¢): Slice Dim 2 (W)
        ax_sag = axes[row_idx, 2]
        slice_sag = data[:, :, w_mid]
        img_sag = 20 * np.log10(np.maximum(slice_sag, 1e-6))
        ax_sag.imshow(img_sag, cmap='gray', aspect=aspect_ratio_long)
        if row_idx == 0: ax_sag.set_title("Sagittal (D-H)\nLongitudinal", fontsize=14, fontweight='bold')
        ax_sag.set_xticks([])
        ax_sag.set_yticks([])

    # ä¿å­˜é«˜æ¸…å¤§å›¾
    save_path = os.path.join(save_dir, 'images', f'{case_name}_Matrix_View.png')
    plt.savefig(save_path, dpi=150)
    plt.close(fig)
    logging.info(f"ä¸‰è§†å›¾çŸ©é˜µå·²ä¿å­˜: {save_path}")

def save_full_volume_nifti_cloned(data, template_path, name, suffix, save_dir):
    """V4.3: å…‹éš†æ¨¡æ¿å…ƒæ•°æ®ä¿å­˜ NIfTI"""
    os.makedirs(os.path.join(save_dir, 'nifti'), exist_ok=True)
    try:
        template_img = nib.load(template_path)
        affine = template_img.affine
        header = template_img.header
    except:
        affine = np.eye(4)
        header = None
        
    nii_img = nib.Nifti1Image(data, affine, header)
    save_path = os.path.join(save_dir, 'nifti', f'{name}_{suffix}.nii')
    nib.save(nii_img, save_path)
    logging.info(f"NIfTIæ–‡ä»¶(æ–¹å‘å·²æ ¡æ­£)å·²ä¿å­˜: {save_path}")

def denormalize_output(normalized_data, max_val):
    """
    [ä¿®æ­£] ä½¿ç”¨å›ºå®šçš„ç‰©ç† Max å€¼è¿›è¡Œåå½’ä¸€åŒ–
    ä¸å†ä¾èµ–å¤–éƒ¨ jsonï¼Œç¡®ä¿è¾“å‡ºæ•°å€¼æ˜¯çœŸå®çš„ç‰©ç†å¼ºåº¦
    """
    data_0_1 = (normalized_data + 1.0) / 2.0
    return data_0_1 * max_val

# def full_inference_8gb(model, input_tensor):
#     """V4.1: 512åˆ†æ®µæ¨ç† (æ˜¾å­˜ä¼˜åŒ–) - ä¿æŒä¸å˜"""
#     _, _, D, H, W = input_tensor.shape
#     mid = 512
#     output_full = np.zeros((D, H, W), dtype=np.float32)
#     model.netG.cuda()
#     with torch.no_grad():
#         # Part 1
#         part1_in = input_tensor[:, :, :mid, :, :].cuda()
#         output_full[:mid, :, :] = model.netG(part1_in).squeeze().cpu().numpy()
#         del part1_in
#         torch.cuda.empty_cache()
#         # Part 2
#         part2_in = input_tensor[:, :, mid:, :, :].cuda()
#         output_full[mid:, :, :] = model.netG(part2_in).squeeze().cpu().numpy()
#         del part2_in
#         torch.cuda.empty_cache()
#     return output_full


def full_inference_overlap(model, input_tensor, overlap=64):
    """
    [V5.0 - 4060 8GB ä¸“ç”¨ç‰ˆ] å¸¦é‡å çš„åˆ‡åˆ†æ¨ç†
    è§£å†³ "ä¸­é—´æœ‰ä¸€æ¡çº¿" çš„è¾¹ç•Œä¼ªå½±é—®é¢˜ã€‚
    åŸç†ï¼šå¤šç®— 64 å±‚ (Overlap) ä½œä¸º Paddingï¼Œæ‹¼æ¥æ—¶ä¸¢å¼ƒè¾¹ç¼˜ã€‚
    """
    b, c, D, H, W = input_tensor.shape
    
    # 1. æ˜¾å­˜ä¸å¤Ÿæ—¶çš„è‡ªåŠ¨ç­–ç•¥ï¼šå¦‚æœæ·±åº¦ < 600ï¼Œç›´æ¥è·‘ (8GB åº”è¯¥èƒ½å‹‰å¼ºåƒä¸‹ 600)
    # å¦‚æœæ·±åº¦å¤ªæ·± (1024)ï¼Œåˆ™å¯åŠ¨åˆ‡åˆ†
    if D < 600:
        model.netG.cuda()
        with torch.no_grad():
            output_full = model.netG(input_tensor.cuda()).squeeze().cpu().numpy()
        return output_full

    # 2. å‡†å¤‡è¾“å‡ºå®¹å™¨
    output_full = np.zeros((D, H, W), dtype=np.float32)
    model.netG.cuda()
    
    mid = D // 2 # 512
    
    with torch.no_grad():
        # ================= Part 1: Top Half =================
        # è¾“å…¥èŒƒå›´: 0 ~ (512 + overlap)
        # ç›®çš„: åªè¦ 0 ~ 512 çš„çº¯å‡€æ•°æ®
        end_idx = min(D, mid + overlap)
        part1_in = input_tensor[:, :, :end_idx, :, :].cuda()
        
        # æ¨ç†
        out1 = model.netG(part1_in).squeeze().cpu().numpy()
        
        # è£å‰ª: åªè¦ [0:mid]
        output_full[:mid, :, :] = out1[:mid, :, :]
        
        # æ¸…æ˜¾å­˜
        del part1_in, out1
        torch.cuda.empty_cache()

        # ================= Part 2: Bottom Half =================
        # è¾“å…¥èŒƒå›´: (512 - overlap) ~ 1024
        # ç›®çš„: åªè¦ 512 ~ 1024 çš„çº¯å‡€æ•°æ®
        start_idx = max(0, mid - overlap)
        part2_in = input_tensor[:, :, start_idx:, :, :].cuda()
        
        # æ¨ç†
        out2 = model.netG(part2_in).squeeze().cpu().numpy()
        
        # è£å‰ª: è¿™é‡Œçš„ out2 é•¿åº¦æ˜¯ (1024 - start_idx)
        # ä¹Ÿå°±æ˜¯ (1024 - (512 - overlap)) = 512 + overlap
        # æˆ‘ä»¬çš„æœ‰æ•ˆæ•°æ®æ˜¯ä» overlap å¼€å§‹çš„ï¼Œå¯¹åº”çš„ç»å¯¹ä½ç½®æ˜¯ 512
        valid_start_rel = mid - start_idx # ç†è®ºä¸Šç­‰äº overlap
        
        output_full[mid:, :, :] = out2[valid_start_rel:, :, :]
        
        # æ¸…æ˜¾å­˜
        del part2_in, out2
        torch.cuda.empty_cache()
        
    return output_full

if __name__ == '__main__':
    opt = TestOptions().parse()
    opt.num_threads = 0
    opt.batch_size = 1
    
    model = create_model(opt)
    model.setup(opt)
    model.eval()
    
    dataset = create_dataset(opt)
    save_root = os.path.join(opt.results_dir, opt.name, f"epoch_{opt.epoch}")
    os.makedirs(save_root, exist_ok=True)
    
    logging.info(f"ğŸš€ V4.6 ä¿®å¤ç‰ˆæµ‹è¯•å¯åŠ¨ | å‚æ•°ä¼ é€’ä¿®å¤ | ç‰©ç†æ•°å€¼æ ¡æ­£")
    
    file_list = dataset.dataset.file_list
    metrics_list = []

    # ç›´æ¥ä» Dataset è·å–æ­£ç¡®çš„ç‰©ç†å®šæ ‡å€¼
    NORM_MAX_INPUT = dataset.dataset.NORM_MAX_INPUT    # 500,000.0
    NORM_MAX_TARGET = dataset.dataset.NORM_MAX_TARGET  # 25,000,000.0

    for i, files in enumerate(tqdm(file_list)):
        case_name = files['case_name']
        template_sq_path = files['p_sq']
        
        # 1. æ ¸å¿ƒæ¨ç†
        # [å…³é”®ä¿®å¤] å‚æ•°é¡ºåºä¿®æ­£: path, slice_objs, max_val
        # è¿™é‡Œçš„ slice(None) è¡¨ç¤ºè¯»å–æ•´ä¸ª 1024x128x128 æ•°æ®
        full_slice = (slice(None), slice(None), slice(None))
        
        input_data = [dataset.dataset._read_and_process(files[k], full_slice, NORM_MAX_INPUT) 
                      for k in ['p_in_n15', 'p_in_000', 'p_in_p15']]
        
        real_lq_tensor = torch.from_numpy(np.stack(input_data, axis=0)).unsqueeze(0).float()
        
        # æ­¤æ—¶ real_lq_tensor æ·±åº¦åº”è¯¥æ˜¯ 1024ï¼Œinference å‡½æ•°å°†æ­£å¸¸å·¥ä½œ
        # fake_norm = full_inference_overlap(model, real_lq_tensor)
        fake_norm = full_inference_overlap(model, real_lq_tensor, overlap=64)

        # 2. åå½’ä¸€åŒ– (ä½¿ç”¨ 2.5e7 çš„åŸºå‡†ï¼Œç¡®ä¿è¾“å‡ºæ˜¯ç™¾ä¸‡çº§æ•°å€¼)
        fake_denorm = denormalize_output(fake_norm, NORM_MAX_TARGET)
        
        # çœŸå€¼ä¹Ÿç”¨åŒæ ·çš„åŸºå‡†è¿˜åŸ
        sq_denorm = denormalize_output(dataset.dataset._read_and_process(files['p_sq'], full_slice, NORM_MAX_TARGET), NORM_MAX_TARGET)
        hq_denorm = denormalize_output(dataset.dataset._read_and_process(files['p_hq'], full_slice, NORM_MAX_TARGET), NORM_MAX_TARGET)
        
        # Input ä¹Ÿè¦è¿˜åŸ (æ³¨æ„ Input çš„åŸºå‡†æ˜¯ 5e5)
        lq_avg_norm = np.mean(np.stack(input_data, axis=0), axis=0)
        lq_avg_denorm = denormalize_output(lq_avg_norm, NORM_MAX_INPUT)

        # 3. æŒ‡æ ‡è®¡ç®—
        m_sq = calc_metrics(torch.from_numpy(fake_denorm).unsqueeze(0).unsqueeze(0), torch.from_numpy(sq_denorm).unsqueeze(0).unsqueeze(0))
        m_hq = calc_metrics(torch.from_numpy(fake_denorm).unsqueeze(0).unsqueeze(0), torch.from_numpy(hq_denorm).unsqueeze(0).unsqueeze(0))

        # 4. ä¿å­˜æ–‡ä»¶
        save_full_volume_nifti_cloned(fake_denorm, template_sq_path, case_name, 'Fake', save_root)
        
        # 5. ä¿å­˜å¯è§†åŒ– (ä½¿ç”¨è¿˜åŸåçš„ç‰©ç†æ•°å€¼ï¼Œlogæ˜¾ç¤ºä¼šå¾ˆæ¸…æ™°)
        save_visual_matrix_png(lq_avg_denorm, fake_denorm, hq_denorm, sq_denorm, case_name, save_root, opt)
        
        metrics_list.append({'Name': case_name, 'PSNR_SQ': m_sq['PSNR'], 'SSIM_SQ': m_sq['SSIM'], 'PSNR_HQ': m_hq['PSNR'], 'SSIM_HQ': m_hq['SSIM']})

    pd.DataFrame(metrics_list).to_csv(os.path.join(save_root, 'metrics_final.csv'), index=False)
    logging.info(f"âœ… å…¨éƒ¨å®Œæˆï¼è¾“å‡ºæ–‡ä»¶å·²ç”Ÿæˆåœ¨ {save_root}")