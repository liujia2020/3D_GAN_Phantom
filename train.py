import time
import os
import torch
import numpy as np
import csv
from PIL import Image
from options.train_options import TrainOptions
from data import create_dataset
from models import create_model
from util.visualizer import Visualizer

# =========================================================================
# è¾…åŠ©å‡½æ•°ï¼šå°† Tensor è½¬ä¸ºå¯è§†åŒ–å›¾ç‰‡ (å•é€šé“ -> ç°åº¦å›¾)
# =========================================================================
def tensor2im_custom(input_image, imtype=np.uint8):
    """
    å°† [-1, 1] çš„ Tensor è½¬æ¢ä¸º [0, 255] çš„ numpy image
    """
    if isinstance(input_image, torch.Tensor):
        image_tensor = input_image.data
    else:
        return input_image
    
    image_numpy = image_tensor[0].cpu().float().numpy()  # å– Batch ä¸­çš„ç¬¬ä¸€å¼ 
    
    # å½¢çŠ¶å¤„ç†: (C, H, W) -> (H, W, C)
    if image_numpy.shape[0] == 1:  # å•é€šé“ (ç°åº¦)
        image_numpy = np.tile(image_numpy, (3, 1, 1))  # å¤åˆ¶æˆ 3 é€šé“æ–¹ä¾¿æ˜¾ç¤º
    
    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0  # [-1,1] -> [0,255]
    return image_numpy.astype(imtype)

# =========================================================================
# è¾…åŠ©å‡½æ•°ï¼šè®¡ç®— PSNR (ç”¨äºç›‘æ§è®­ç»ƒè´¨é‡)
# =========================================================================
def calculate_psnr(img1, img2):
    mse = np.mean((img1 - img2) ** 2)
    if mse == 0:
        return 100
    PIXEL_MAX = 255.0
    return 20 * np.log10(PIXEL_MAX / np.sqrt(mse))

# =========================================================================
# ä¸»è®­ç»ƒé€»è¾‘
# =========================================================================
if __name__ == '__main__':
    # 1. è§£æå‚æ•°
    opt = TrainOptions().parse()
    
    # ------------------------------------------------
    # [å¢å¼ºåŠŸèƒ½ 1] è¯¦ç»†ä¿¡æ¯æ‰“å°ä¸å›ºåŒ–
    # ------------------------------------------------
    print("="*80)
    print(f"ğŸš€ è®­ç»ƒå¯åŠ¨: {opt.name}")
    print(f"ğŸ“‚ æ•°æ®é›†è·¯å¾„: {opt.dataroot}")
    print(f"   è¾“å…¥æ–‡ä»¶å¤¹ (LQ): {opt.dir_lq}")
    print(f"   çœŸå€¼æ–‡ä»¶å¤¹ (SQ): {opt.dir_sq}")
    print(f"ğŸ”§ æ ¸å¿ƒå‚æ•°: Batch={opt.batch_size}, L1_W={opt.lambda_pixel}, GAN_W={opt.lambda_gan}, VGG_W={opt.lambda_perceptual}")
    print("="*80)

    # 2. åˆ›å»ºæ•°æ®é›†
    dataset = create_dataset(opt)
    dataset_size = len(dataset)
    print(f'ğŸ“Š è®­ç»ƒé›†å›¾ç‰‡æ€»æ•° = {dataset_size}')

    # 3. åˆ›å»ºæ¨¡å‹
    model = create_model(opt)
    model.setup(opt)
    
    # 4. åˆå§‹åŒ– CSV æ—¥å¿—æ–‡ä»¶
    # log_path: ./checkpoints/å®éªŒå/loss_log.csv
    expr_dir = os.path.join(opt.checkpoints_dir, opt.name)
    if not os.path.exists(expr_dir):
        os.makedirs(expr_dir)
    log_name = os.path.join(expr_dir, 'loss_log.csv')
    
    # å¦‚æœæ˜¯æ–°è®­ç»ƒï¼Œå†™å…¥è¡¨å¤´ï¼›å¦‚æœæ˜¯ç»­è®­ï¼Œç›´æ¥è¿½åŠ 
    if not opt.continue_train or not os.path.exists(log_name):
        with open(log_name, mode='w', newline='') as f:
            writer = csv.writer(f)
            # è¡¨å¤´ï¼šEpoch, è€—æ—¶, Gæ€»Loss, Dæ€»Loss, L1é¡¹, GANé¡¹, VGGé¡¹, PSNR, å­¦ä¹ ç‡
            writer.writerow(['Epoch', 'Time(s)', 'G_Total', 'D_Total', 'G_L1', 'G_GAN', 'G_VGG', 'PSNR', 'LR'])
        print(f"ğŸ“ åˆ›å»ºæ–°æ—¥å¿—æ–‡ä»¶: {log_name}")
    else:
        print(f"ğŸ”„ è¿½åŠ åˆ°ç°æœ‰æ—¥å¿—æ–‡ä»¶: {log_name}")

    # 5. è®­ç»ƒå¾ªç¯
    total_iters = 0                
    
    # ç¡®å®šèµ·æ­¢ Epoch
    start_epoch = opt.epoch_count
    end_epoch = opt.n_epochs + opt.n_epochs_decay

    for epoch in range(start_epoch, end_epoch + 1):
        epoch_start_time = time.time()
        iter_data_time = time.time()
        epoch_iter = 0
        model.update_learning_rate()

        # ç”¨äºç»Ÿè®¡æœ¬ Epoch çš„å¹³å‡ Loss
        epoch_losses = {'G_Total': 0.0, 'D_Total': 0.0, 'G_L1': 0.0, 'G_GAN': 0.0, 'G_VGG': 0.0, 'PSNR': 0.0}
        num_batches = 0

        print(f'\nğŸ”µ Epoch {epoch}/{end_epoch} å¼€å§‹...')

        for i, data in enumerate(dataset):
            iter_start_time = time.time()
            if total_iters % opt.print_freq == 0:
                t_data = iter_start_time - iter_data_time

            total_iters += opt.batch_size
            epoch_iter += opt.batch_size
            
            # --- æ ¸å¿ƒè®­ç»ƒæ­¥ ---
            model.set_input(data)
            model.optimize_parameters()
            
            # --- æ”¶é›† Loss æ•°æ® ---
            losses = model.get_current_losses()
            # æ³¨æ„ï¼šè¿™é‡Œçš„ key è¦å’Œä½  augan_model.py é‡Œå®šä¹‰çš„ loss_names å¯¹åº”
            # é€šå¸¸æ˜¯ G_GAN, G_Pixel(å³L1), D_Real, D_Fake
            # ä¸ºäº†é€šç”¨æ€§ï¼Œæˆ‘è¿™é‡Œåšä¸€ä¸ªæ˜ å°„å°è¯•ï¼Œå¦‚æœå–ä¸åˆ°å°±å¡« 0
            g_loss = losses.get('G_GAN', 0) + losses.get('G_Pixel', 0) + losses.get('G_Perceptual', 0)
            d_loss = losses.get('D_Real', 0) + losses.get('D_Fake', 0)
            
            epoch_losses['G_Total'] += g_loss
            epoch_losses['D_Total'] += d_loss
            epoch_losses['G_L1']    += losses.get('G_Pixel', 0)
            epoch_losses['G_GAN']   += losses.get('G_GAN', 0)
            epoch_losses['G_VGG']   += losses.get('G_Perceptual', 0)

            # --- è®¡ç®— Training PSNR (ä»…ä¾›å‚è€ƒ) ---
            # è·å–å½“å‰ batch çš„å›¾åƒ
            model.compute_visuals()
            visuals = model.get_current_visuals()
            # visuals é‡Œé€šå¸¸æœ‰ real_lq, fake_hq, real_sq (å¯¹åº” augan_model.py çš„ visual_names)
            # å¦‚æœåå­—ä¸ä¸€æ ·ï¼Œè¿™é‡Œä¼šè‡ªåŠ¨ fallback
            fake_im = tensor2im_custom(visuals.get('fake_hq', list(visuals.values())[0]))
            real_im = tensor2im_custom(visuals.get('real_sq', list(visuals.values())[1]))
            epoch_losses['PSNR'] += calculate_psnr(fake_im, real_im)

            num_batches += 1

            # --- å±å¹•æ‰“å° (Print Freq) ---
            if total_iters % opt.print_freq == 0:
                t_comp = (time.time() - iter_start_time) / opt.batch_size
                print(f"Epoch: {epoch} | Iters: {epoch_iter} | Time: {t_comp:.3f}s | "
                      f"G_L1: {losses.get('G_Pixel', 0):.4f} | G_GAN: {losses.get('G_GAN', 0):.4f}")

            # --- ä¿å­˜æœ€æ–°æ¨¡å‹ (Freq) ---
            if total_iters % opt.save_latest_freq == 0:
                print(f'ğŸ’¾ ä¿å­˜ latest æ¨¡å‹ (epoch {epoch}, iters {total_iters})')
                model.save_networks('latest')

            iter_data_time = time.time()

        # =================================================
        # End of Epoch: ç»Ÿè®¡ã€æ—¥å¿—ã€ç»˜å›¾
        # =================================================
        
        # 1. è®¡ç®—å¹³å‡å€¼
        for k in epoch_losses:
            epoch_losses[k] /= max(num_batches, 1)
        
        time_taken = time.time() - epoch_start_time
        current_lr = model.optimizers[0].param_groups[0]['lr']

        # 2. å†™å…¥ CSV
        with open(log_name, mode='a', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([epoch, 
                             f"{time_taken:.1f}", 
                             f"{epoch_losses['G_Total']:.4f}", 
                             f"{epoch_losses['D_Total']:.4f}", 
                             f"{epoch_losses['G_L1']:.4f}", 
                             f"{epoch_losses['G_GAN']:.4f}", 
                             f"{epoch_losses['G_VGG']:.4f}", 
                             f"{epoch_losses['PSNR']:.2f}", 
                             f"{current_lr:.6f}"])
        print(f"âœ… Epoch {epoch} ç»“æŸ. Time: {time_taken:.1f}s, Avg PSNR: {epoch_losses['PSNR']:.2f} dB")

        # 3. [å¢å¼ºåŠŸèƒ½ 2] ç”Ÿæˆå¯¹æ¯”å›¾ (Input | Fake | Truth)
        # å–æœ¬ Epoch æœ€åä¸€ä¸ª Batch çš„æ•°æ®æ¥ç”»å›¾
        visuals = model.get_current_visuals()
        
        # æå–å›¾ç‰‡ (ç¡®ä¿ key ä¸ augan_model.py ä¸€è‡´)
        # ä½ çš„ dataset è¿”å›çš„æ˜¯ lq, sq. augan_model set_input åº”è¯¥æ˜ å°„ä¸ºäº† real_lq, real_sq
        img_lq = tensor2im_custom(visuals.get('real_lq'))
        img_fake = tensor2im_custom(visuals.get('fake_hq'))
        img_sq = tensor2im_custom(visuals.get('real_sq'))
        
        # æ‹¼å›¾ (æ¨ªå‘æ‹¼æ¥)
        # å½¢çŠ¶: [H, W, 3]
        h, w, c = img_lq.shape
        combined_image = np.concatenate([img_lq, img_fake, img_sq], axis=1) # 1 æ˜¯å®½åº¦æ–¹å‘
        
        # ä¿å­˜
        img_dir = os.path.join(expr_dir, 'web_images')
        if not os.path.exists(img_dir):
            os.makedirs(img_dir)
        
        save_path = os.path.join(img_dir, f'epoch_{epoch:03d}_comparison.png')
        Image.fromarray(combined_image).save(save_path)
        print(f"ğŸ–¼ï¸ ä¿å­˜å¯¹æ¯”å›¾: {save_path}")

        # 4. ä¿å­˜æ¨¡å‹ (æŒ‰ Epoch)
        if epoch % opt.save_epoch_freq == 0:
            print(f'ğŸ’¾ ä¿å­˜ epoch {epoch} æ¨¡å‹...')
            model.save_networks('latest')
            model.save_networks(epoch)

    print("ğŸ æ‰€æœ‰è®­ç»ƒå®Œæˆ!")

# """
# AUGAN 3D è®­ç»ƒä¸»å…¥å£è„šæœ¬ (V9.5 - 5å›¾å¯è§†åŒ–ä¿®å¤ç‰ˆ)
# ä¿®æ”¹è¯´æ˜ï¼š
# 1. ä¿®å¤å¯è§†åŒ–ï¼šç°åœ¨è°ƒç”¨ util.save_training_imagesï¼Œæ˜¾ç¤º 5 å¼ å›¾ (å« Diff Map)ã€‚
# 2. ä¿ç•™ CSV å¯¼å‡ºåŠŸèƒ½ã€‚
# """
# import time
# import os
# import torch
# import numpy as np
# import random
# import csv
# from tqdm import tqdm
# from torch.utils.tensorboard import SummaryWriter
# import matplotlib
# matplotlib.use('Agg') 
# import matplotlib.pyplot as plt
# import nibabel as nib 

# # [å…³é”®] å¯¼å…¥é«˜çº§å¯è§†åŒ–å‡½æ•°
# from util import save_training_images

# try:
#     from options.train_options import TrainOptions
# except ImportError:
#     import sys
#     sys.path.append('.')
#     from options.train_options import TrainOptions

# from data import create_dataset
# from models import create_model

# def set_seed(seed):
#     torch.manual_seed(seed)
#     torch.cuda.manual_seed_all(seed)
#     np.random.seed(seed)
#     random.seed(seed)
#     torch.backends.cudnn.deterministic = True

# def print_training_summary(opt, dataset, model):
#     device = torch.device('cuda:{}'.format(opt.gpu_ids[0])) if opt.gpu_ids else torch.device('cpu')
#     print("\n" + "="*80)
#     print(f"{'ğŸš€ AUGAN TRAINING CONFIGURATION':^80}")
#     print("="*80)
#     print(f"  - Device:        {device}")
#     print(f"  - Data Root:     {opt.dataroot}")
#     print(f"  - Dataset Size:  {len(dataset)} volumes")
#     print(f"  - Batch Size:    {opt.batch_size}")
#     print(f"  - Model:         G={opt.netG}, D={opt.netD}")
#     print("="*80 + "\n")

# def print_epoch_report(epoch, total_epochs, epoch_time, losses_avg, lr_G, lr_D):
#     print('-' * 80)
#     print(f'END OF EPOCH {epoch} / {total_epochs} \t Time Taken: {epoch_time:.0f} sec')
#     print(f'  Learning Rates: \t G_lr = {lr_G:.7f} | D_lr = {lr_D:.7f}')
#     for k, v in losses_avg.items():
#         if 'G_' in k: print(f'      {k}: \t {v:.4f}')
#     print('-' * 80 + '\n')

# # ==============================================================================
# # [ä¸»ç¨‹åº]
# # ==============================================================================
# if __name__ == '__main__':
#     opt_driver = TrainOptions() 
#     opt = opt_driver.parse()    
#     set_seed(42)
    
#     log_dir = os.path.join(opt.checkpoints_dir, opt.name, 'logs')
#     img_save_dir = os.path.join(opt.checkpoints_dir, opt.name, 'web_images')
#     os.makedirs(log_dir, exist_ok=True)
#     os.makedirs(img_save_dir, exist_ok=True)
    
#     csv_log_path = os.path.join(opt.checkpoints_dir, opt.name, 'loss_log.csv')
#     writer = SummaryWriter(log_dir=log_dir)

#     dataset = create_dataset(opt)
#     model = create_model(opt)
#     model.setup(opt)
    
#     print_training_summary(opt, dataset, model)
    
#     total_iters = 0                
#     total_epochs = opt.n_epochs + opt.n_epochs_decay
    
#     print("ğŸ“¸ Saving initial sample (Step 0 check)...")
#     init_batch = next(iter(dataset))
#     model.set_input(init_batch)
#     model.forward()
    
#     # [ä¿®å¤] è°ƒç”¨é«˜çº§å¯è§†åŒ– (æ˜¾ç¤º 5 å¼ å›¾)
#     save_training_images(
#         model.real_lq, model.fake_hq, model.real_hq, model.real_sq, 
#         0, img_save_dir, dynamic_range=60
#     )
    
#     # --- è®­ç»ƒå¾ªç¯ ---
#     for epoch in range(opt.epoch_count, total_epochs + 1):
#         epoch_start_time = time.time()
#         epoch_losses = {} 
#         epoch_iter_count = 0
        
#         print(f"\nStart Epoch {epoch} / {total_epochs}")
#         progress_bar = tqdm(dataset, desc=f"Epoch {epoch}", unit="batch")

#         for i, data in enumerate(progress_bar):
#             total_iters += opt.batch_size
#             epoch_iter_count += 1
            
#             model.set_input(data)         
#             model.optimize_parameters()   
            
#             current_losses = model.get_current_losses()
#             for k, v in current_losses.items():
#                 epoch_losses[k] = epoch_losses.get(k, 0.0) + v

#             if total_iters % opt.print_freq == 0:    
#                 loss_display = {k.replace('G_', ''): f"{v:.3f}" for k, v in current_losses.items() if 'G_' in k}
#                 progress_bar.set_postfix(**loss_display)
#                 for k, v in current_losses.items():
#                     writer.add_scalar(f'Loss_Step/{k}', v, total_iters)
                    
#         # è®¡ç®—å¹³å‡ Loss
#         avg_losses = {k: v / epoch_iter_count for k, v in epoch_losses.items()}
#         for k, v in avg_losses.items():
#             writer.add_scalar(f'Loss_Epoch/{k}', v, epoch)
            
#         lr_G = model.optimizers[0].param_groups[0]['lr']
#         lr_D = model.optimizers[1].param_groups[0]['lr']
        
#         print_epoch_report(epoch, total_epochs, time.time() - epoch_start_time, avg_losses, lr_G, lr_D)
        
#         # --- CSV ä¿å­˜ ---
#         try:
#             write_header = not os.path.exists(csv_log_path)
#             with open(csv_log_path, mode='a', newline='') as f:
#                 fieldnames = ['epoch'] + sorted(avg_losses.keys())
#                 writer_csv = csv.DictWriter(f, fieldnames=fieldnames)
#                 if write_header: writer_csv.writeheader()
#                 row = {'epoch': epoch}; row.update(avg_losses)
#                 writer_csv.writerow(row)
#             print(f"  ğŸ“ˆ CSV Log Saved")
#         except Exception as e:
#             print(f"  âš ï¸ CSV Error: {e}")

#         # --- [ä¿®å¤] å¯è§†åŒ– ---
#         save_training_images(
#             model.real_lq, model.fake_hq, model.real_hq, model.real_sq, 
#             epoch, img_save_dir, dynamic_range=60
#         )
        
#         if epoch % opt.save_epoch_freq == 0:
#             print(f'ğŸ’¾ Saving checkpoints at epoch {epoch}')
#             model.save_networks('latest')
#             model.save_networks(epoch)

#         model.update_learning_rate() 
        
#     writer.close()
#     print("ğŸ‰ Training Finished!")